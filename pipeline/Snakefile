"""
Define all rules (to define commands):

- download data
- download trained models and index
- query index
- train index
"""
from os.path import join as pjoin

configfile: "pipeline/config/params.yaml"

KMER = config["kmer_size"]
SUBSET = config["subset"]

include: "workflow/rules/create_fcgr.smk"
# include: "workflow/rules/create_index.smk"
include: "workflow/rules/download_gold_standard.smk"
# include: "workflow/rules/query_index.smk"
# include: "workflow/rules/stats_assembly.smk"
include: "workflow/rules/download_bacteria.smk"

##### target rules #####

### ---- Download data ----

# 661k bacterial dataset
rule download_bacteria:
    input:
        f"data/batches_661k_bacteria_{SUBSET}.flag"

# Gold standar datasets
PATH_DATASET_GOLD_STANDARD = Path("data/gold_standard")

ACCESSIONS_NCBI={
    # "NCTC3000": "PRJEB6403",
    "GEBA": "PRJNA30815",
    # "FDA-ARGOS": "PRJNA231221",
}
datasets = list(ACCESSIONS_NCBI.keys())

rule download_gold_standard:
    input: 
        expand(PATH_DATASET_GOLD_STANDARD.joinpath("{dataset}/ncbi_dataset.zip"), dataset=datasets),
        expand(PATH_DATASET_GOLD_STANDARD.joinpath("{dataset}/list_paths.txt"), dataset=datasets)

### ---- FCGR ----
def load_batches(subset):
    list_files=[]
    with open(f"data/batches_661k_bacteria_{subset}.txt") as fp:
        
        for line in fp.readlines():
            tarxz = line.replace("\n","").strip().split(" ")[-1]
            name = tarxz.replace(".tar.xz","")
            list_files.append(name)
    return list_files

list_files=load_batches(SUBSET)
print(list_files)
rule fcgr:
    input:
        expand( pjoin(DIRFCGR,"{tarfile}_fcgr.flag"), tarfile=list_files)

### ---- Cross Validation ---- 

## Autoencoder


## Metric Learning